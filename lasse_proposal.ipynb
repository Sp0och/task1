{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and read in as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "############# read training and test data from the files #############\n",
    "X_train = pd.read_csv(\"X_train.csv\").to_numpy()\n",
    "X_train = X_train[:,1:]  #remove the id column\n",
    "y_train = pd.read_csv(\"y_train.csv\").to_numpy()\n",
    "y_train = y_train[:,1]\n",
    "X_test = pd.read_csv(\"X_test.csv\").to_numpy()\n",
    "X_test = X_test[:,1:]  #remove the id column\n",
    "\n",
    "X_train0 = X_train\n",
    "y_train0 = y_train\n",
    "X_test0 = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of redundant features as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first remove features with very low variance\n",
    "normalized_std_threshold = 0.001\n",
    "normalized_std = np.zeros(X_train.shape[1])\n",
    "means_abs = np.zeros(X_train.shape[1])\n",
    "for i in range(X_train.shape[1]):\n",
    "    vec = X_train[:,i]\n",
    "    means_abs[i] = np.abs(np.mean(vec[~np.isnan(vec)])) + 1\n",
    "    normalized_std[i] = np.std(vec[~np.isnan(vec)])/means_abs[i]\n",
    "\n",
    "mask = np.ones(len(normalized_std), dtype=bool)\n",
    "mask[normalized_std < normalized_std_threshold] = False\n",
    "\n",
    "X_train = X_train[:, mask]\n",
    "X_test = X_test[:, mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling as before and then outlier rejection before initial imputation\n",
    "### Imputation perhaps with MICE already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scaling\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# proposal: first outlier removal:\n",
    "############# Outlier detection: IsolationForest #############\n",
    "outlier_det = IsolationForest(random_state=0, contamination=0.05, max_features=0.5).fit(X_train_sc_imp0)\n",
    "anomaly_score = outlier_det.decision_function(X_train_sc_imp0)\n",
    "\n",
    "### to determine the contamination parameter, I propose to first plot the anomaly score function and then decide on the\n",
    "### the contamination ratio based on visual thresholding.\n",
    "### HERE WE NEED TO VISUALLY DETERMINE A THRESHOLD:\n",
    "anomaly_threshold = 0.0\n",
    "plt.plot(anomaly_score)\n",
    "plt.plot(np.ones(anomaly_score.shape)*anomaly_threshold)\n",
    "plt.ylabel('anomaly score')\n",
    "plt.show()\n",
    "\n",
    "outliers=np.where(anomaly_score < anomaly_threshold)\n",
    "\n",
    "mask = np.ones(len(anomaly_score), dtype=bool)\n",
    "mask[outliers[0]] = False\n",
    "X_train_cl0 = X_train_sc_imp0[mask,:]\n",
    "y_train_cl0 = y_train[mask]\n",
    "\n",
    "print('Size of training set after initial preprocessing:', X_train_cl0.shape)\n",
    "\n",
    "# and then imputation\n",
    "\n",
    "############ Imputation #############\n",
    "# imputer = KNNImputer(n_neighbors=50) \n",
    "# X_train_sc_imp0 = imputer.fit_transform(X_train_scaled)\n",
    "# X_test_sc_imp0 = imputer.fit_transform(X_test_scaled)\n",
    "\n",
    "Imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "X_train_sc_imp0 = Imputer.fit_transform(X_train_scaled)\n",
    "X_test_sc_imp0 = Imputer.fit_transform(X_test_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Feature selection using ExtraTreesRegressor #############\n",
    "\n",
    "selector = ExtraTreesRegressor(n_estimators=50, random_state=0, min_samples_split=0.02, max_samples=0.9)\n",
    "selector = selector.fit(X_train_cl0, y_train_cl0)\n",
    "selector.feature_importances_  \n",
    "\n",
    "support = np.where(selector.feature_importances_ > 1e-3)[0]\n",
    "print('number of selected features:', len(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated Feature Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# corr_threshold = 0.9\n",
    "\n",
    "# corr_matrix = np.abs(np.corrcoef(X_train_cl0[:,support], rowvar=False))\n",
    "# mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "# corr_mat_triu = np.ma.masked_array(corr_matrix, mask=mask)\n",
    "\n",
    "# # plt.imshow(corr_matrix, cmap='hot', interpolation='nearest')\n",
    "# # plt.show()\n",
    "\n",
    "# drop_features = [c for c in range(corr_mat_triu.shape[1]) if any(corr_mat_triu[:,c] > corr_threshold)]\n",
    "# corr_mask = np.ones(corr_mat_triu.shape[1], dtype=bool)\n",
    "# corr_mask[drop_features] = False\n",
    "# support = support[corr_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Rejection and Data Imputation with relevant features\n",
    "## Again with the order Outlier detection -> Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier Rejection 2:\n",
    "outlierdet = IsolationForest(random_state=0, contamination=0.05, max_features=0.5).fit(X_train_sc_imp)\n",
    "anomaly_score2 = outlierdet.decision_function(X_train_sc_imp)\n",
    "\n",
    "anomaly_threshold2 = 0.02\n",
    "plt.plot(anomaly_score2)\n",
    "plt.plot(np.ones(anomaly_score2.shape)*anomaly_threshold2)\n",
    "plt.ylabel('anomaly score after feature sel')\n",
    "plt.show()\n",
    "\n",
    "outliers=np.where(anomaly_score2 < anomaly_threshold2)\n",
    "\n",
    "mask = np.ones(len(anomaly_score2), dtype=bool)\n",
    "mask[outliers[0]] = False\n",
    "X_train_cl = X_train_sc_imp[mask,:]\n",
    "y_train_cl = y_train_sel[mask]\n",
    "\n",
    "print('Shape of the training set:', X_train_cl.shape)\n",
    "\n",
    "\n",
    "#Imputation 2:\n",
    "#############  #############\n",
    "X_train_sel = X_train_scaled[:,support]\n",
    "X_train_sel = X_train_sel[mask, :]\n",
    "X_test_sel = X_test_scaled[:,support]\n",
    "y_train_sel = y_train[mask]\n",
    "\n",
    "############# MICE imputation #############\n",
    "\n",
    "imp_mean = IterativeImputer(random_state=0, n_nearest_features = None, sample_posterior = False, max_iter=100) \n",
    "X_train_sc_imp = imp_mean.fit_transform(X_train_sel)\n",
    "X_test_sc_imp = imp_mean.fit_transform(X_test_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "## Regression models and combination thereof\n",
    "### Just implement it differently than the vectorized approach\n",
    "### What is this split between decision trees and the other methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Regression using a decision tree #############\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "reg_model = DecisionTreeRegressor(random_state=0, min_samples_split=0.01)\n",
    "n_scores = cross_val_score(reg_model, X_train_cl, y_train_cl, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('MAE: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "run_list = [0,0,1,0,1]\n",
    "\n",
    "############# Regression using a random forest #############\n",
    "if run_list[0]:\n",
    "    rf_reg = ExtraTreesRegressor(n_estimators=100, random_state=0, min_samples_split=5, max_samples=None)\n",
    "    n_scores = cross_val_score(rf_reg, X_train_cl, y_train_cl, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    print('ExtraTrees: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "    regressor = rf_reg.fit(X_train_cl, y_train_cl)\n",
    "    y_predicted_rf = regressor.predict(X_test_sc_imp)\n",
    "\n",
    "\n",
    "\n",
    "############# Regression using HistGradientBoostingRegressor #############\n",
    "if run_list[1]:\n",
    "    HGB_reg = HistGradientBoostingRegressor(max_iter=200, learning_rate = 0.1, l2_regularization = 10)\n",
    "    n_scores = cross_val_score(HGB_reg, X_train_cl, y_train_cl, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    print('HBG: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "    regressor = HGB_reg.fit(X_train_cl, y_train_cl)\n",
    "    y_predicted_hgb = regressor.predict(X_test_sc_imp)\n",
    "\n",
    "############# Regression using SVR #############\n",
    "if run_list[2]:\n",
    "    SVR_reg = SVR(kernel='rbf', degree=50, gamma=0.011, coef0=0.0, tol=0.1, C=100, epsilon=0.1, shrinking=True, cache_size=200)\n",
    "    n_scores = cross_val_score(SVR_reg, X_train_cl, y_train_cl, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    print('SVR: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "    regressor = SVR_reg.fit(X_train_cl, y_train_cl)\n",
    "    y_predicted_svr = regressor.predict(X_test_sc_imp)\n",
    "    \n",
    "############# Regression using adaboost #############\n",
    "if run_list[3]:\n",
    "    ada_regr = AdaBoostRegressor(random_state=0, n_estimators=1000, loss='square', learning_rate=0.5)\n",
    "    n_scores = cross_val_score(ada_regr, X_train_cl, y_train_cl, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    print('Ada: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "############# Regression using MLP #############\n",
    "if run_list[4]:\n",
    "    MLP_reg = MLPRegressor(random_state=10, max_iter=10000,activation='tanh',solver='sgd',alpha=10, hidden_layer_sizes=(200))\n",
    "    n_scores = cross_val_score(MLP_reg, X_train_cl, y_train_cl, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    print('MLP: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "    regressor = MLP_reg.fit(X_train_cl, y_train_cl)\n",
    "    y_predicted_mlp = regressor.predict(X_test_sc_imp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Predictions and write to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Write out the predictions to a csv file #############\n",
    "y_predicted = 1/2*(y_predicted_svr + y_predicted_mlp)\n",
    "d = {'id': range(len(y_predicted)), 'y': y_predicted}\n",
    "y_predicted_df = pd.DataFrame(data=d)\n",
    "y_predicted_df.to_csv(\"AMLES_submission_v6.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
